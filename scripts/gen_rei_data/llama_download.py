#!/usr/bin/env python3
"""
llama_download.py
ä¸“é—¨ç”¨äºä¸‹è½½ LLaMA æ¨¡å‹ï¼ˆåŒ…æ‹¬ gated æ¨¡å‹ï¼Œå¦‚ Llama-3.1-8B-Instructï¼‰

åŠŸèƒ½ï¼š
- è‡ªåŠ¨ç™»å½• HuggingFaceï¼ˆå¯ä¼  token æˆ–ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
- è‡ªåŠ¨æ£€æµ‹è®¿é—®æƒé™
- è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆtokenizer + weightsï¼‰
- å¸¦è¯¦ç»†é”™è¯¯æç¤ºï¼ˆä¾‹å¦‚ 401 æ— æƒé™ï¼‰
- æ–­ç‚¹ç»­ä¼ 
"""

import argparse
import os
from huggingface_hub import HfApi, snapshot_download, login, HfHubHTTPError, GatedRepoError


def print_bar():
    print("=" * 80)


def llama_download(model_name, token=None, local_dir="llama_models", revision="main"):
    print_bar()
    print(f"ğŸš€ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    print_bar()

    # -------------------------
    # Step 1: ç™»å½• HuggingFace
    # -------------------------
    if token is None:
        token = os.environ.get("HUGGINGFACE_HUB_TOKEN", None)

    if token is None:
        print("âŒ æœªæä¾› tokenï¼Œä¹Ÿæ²¡æœ‰ä»ç¯å¢ƒå˜é‡æ‰¾åˆ° HUGGINGFACE_HUB_TOKENã€‚")
        print("   ä½ å¯ä»¥è¿™æ ·è¿è¡Œï¼š")
        print("       HUGGINGFACE_HUB_TOKEN=xxx python llama_download.py --model meta-llama/Llama-3.1-8B-Instruct")
        return

    try:
        login(token=token)
        print("ğŸ” HuggingFace ç™»å½•æˆåŠŸï¼")
    except Exception as e:
        print("âŒ ç™»å½•å¤±è´¥ï¼š", e)
        return

    # -------------------------
    # Step 2: æ£€æŸ¥æƒé™
    # -------------------------
    api = HfApi()
    try:
        print("ğŸ” æ­£åœ¨æ£€æŸ¥è®¿é—®æƒé™â€¦")
        api.model_info(model_name, token=token)
        print("âœ… è®¿é—®æƒé™æ­£å¸¸ï¼Œå¯ä»¥ä¸‹è½½ã€‚")
    except GatedRepoError as e:
        print("âŒ ä½ æ²¡æœ‰æƒé™è®¿é—®è¯¥ gated æ¨¡å‹ï¼š")
        print(e)
        print("\nè¯·å‰å¾€ HF æ¨¡å‹é¡µé¢ç”³è¯·è®¿é—®æƒé™ï¼š")
        print(f"ğŸ‘‰ https://huggingface.co/{model_name}")
        return
    except HfHubHTTPError as e:
        print("âŒ è®¿é—® HuggingFace å¤±è´¥ï¼š", e)
        return
    except Exception as e:
        print("âŒ æœªçŸ¥é”™è¯¯ï¼š", e)
        return

    # -------------------------
    # Step 3: ä¸‹è½½æ¨¡å‹
    # -------------------------
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰â€¦")
    try:
        snapshot_download(
            repo_id=model_name,
            token=token,
            revision=revision,
            local_dir=local_dir,
            local_dir_use_symlinks=False,  # æ–¹ä¾¿å¤åˆ¶
            resume_download=True
        )
        print_bar()
        print(f"ğŸ‰ æ¨¡å‹å·²æˆåŠŸä¸‹è½½åˆ°: {local_dir}/{model_name}")
        print("ä½ å¯ä»¥ç›´æ¥ç”¨ transformers åŠ è½½è¯¥ç›®å½•ã€‚")
        print_bar()

    except Exception as e:
        print("âŒ ä¸‹è½½å¤±è´¥ï¼š", e)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True,
                        help="æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ meta-llama/Llama-3.1-8B-Instruct")
    parser.add_argument("--token", type=str, default=None,
                        help="å¯é€‰ï¼šHF tokenï¼Œä¸æä¾›åˆ™è¯»å–ç¯å¢ƒå˜é‡ HUGGINGFACE_HUB_TOKEN")
    parser.add_argument("--out", type=str, default="llama_models",
                        help="ä¸‹è½½ç›®å½•")
    parser.add_argument("--revision", type=str, default="main",
                        help="æ¨¡å‹ revision æˆ–åˆ†æ”¯")
    args = parser.parse_args()

    llama_download(
        model_name=args.model,
        token=args.token,
        local_dir=args.out,
        revision=args.revision
    )


if __name__ == "__main__":
    main()
